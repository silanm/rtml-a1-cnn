{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A1: CNNs\n",
    "\n",
    "In this lab, we will develop PyTorch implementations of AlexNet and GoogleLeNet from scratch and compare them on CIFAR-10.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some important terms for PyTorch and deep learning with CNNs\n",
    "\n",
    "**Tensor**: a multidimensional matrix for calculation. Inputs, outputs, and weights are all stored as tensors. A CNN will have an \"input\" tensor as input (one or more images), and an \"output\" tensor as the output.\n",
    "\n",
    "**Kernel**: a filter tensor or weight tensor, most often used in a convolution computation.\n",
    "\n",
    "**Channel**: when we deal with 2D feature maps, the third-from-last dimension indexes the channel or depth dimension. A 2D image stored as a tensor has three dimensions: channel, row, and column.\n",
    "\n",
    "**Feature**: could refer to the result of a convolution operation (feature \"map\") or a hand-crafted input to a model or a unit in a linear/dense/fully-connected layer.\n",
    "\n",
    "**Feature extraction**: the process of transforming raw data into numerical features that concentrate and/or preserve the useful information in the raw data.\n",
    "\n",
    "**Stride**: The jump necessary to go from one element to the next when performing an operation on an input tensor.\n",
    "\n",
    "**Padding**: Additional elements added around the boundaries of a tensor to allow convolution operations or other operations to preserve size. Padding is usually with 0 elements, but other methods include copy-border and mirror reflection.\n",
    "\n",
    "In PyTorch, we do not have to specify the input size to a convolution operation, but we do need to specify the number of input channels and the kernel size and stride. Any specific operation will lead to a specific ouptut size depending on the input size. For dense or fully-connected layers, however, we need to specify the number of input features as well as the number of output features, so it is necessary to understand how to calculate how tensor operations affect the output tensor size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the number of parameters and output tensor size for tensor operations\n",
    "\n",
    "### Convolutional layer parameters\n",
    "\n",
    "The number of parameters $k$ in a kernel for a 2D convolution operation is\n",
    "\n",
    "$$k = k_w k_h i_c,$$\n",
    "\n",
    "where $k_w$ is the width of the kernel, $k_h$ is the width of kernel, and $i_c$ is the number input channels.\n",
    "If we have $o_c$ kernels producing $o_c$ output channels, the total number of parameters/weights can be calculated as\n",
    "\n",
    "$$n_p = k o_c = k_w k_h i_c o_c.$$\n",
    "\n",
    "The bias weight in a convolution operation is optional. It's not needed if you apply normalization procedures such as\n",
    "batch normalization (almost always done in modern networks), but it is important if you're not using batch normalization.\n",
    "In that case, the number of biases is equal to the number of kernels:\n",
    "\n",
    "$$n_p = k_w k_h i_c o_c + o_c.$$\n",
    "\n",
    "### Fully connected layer parameters\n",
    "\n",
    "PyTorch separates the linear operation from the nonlinear activation function in a fully connected layer. The linear operation\n",
    "is called a \"linear\" layer, then you have to add the activation function separately. Other frameworks such as keras use the term\n",
    "\"dense layer\" for a fully connected layer including the nonlinear activation function.\n",
    "\n",
    "The number of weights $s_w$ in a linear layer is\n",
    "\n",
    "$$s_w = i_f o_f$$\n",
    "\n",
    "or\n",
    "\n",
    "$$s_w = i_f o_f + o_f$$\n",
    "\n",
    "if we include a bias weight (again, not necessary if we are going to follow up with batch normalization).\n",
    "\n",
    "It is useful to calculate the total number of parameters across all layers in a network to understand how statistically efficient it's going to be.\n",
    "\n",
    "### Convolutional layer output tensor size\n",
    "\n",
    "If we have an input tensor of size $w \\times h \\times c$ and want to perform a convolution with a $k_w \\times k_h$ kernel with padding $p$ and stride $s$,\n",
    "we can calculate the output tensor size as\n",
    "\n",
    "$$\\lfloor \\frac{w+2p-k_w}{s} + 1 \\rfloor \\times \\lfloor \\frac{h+2p-k_h}{s} + 1 \\rfloor.$$\n",
    "\n",
    "For example, in AlexNet, the input image in the first layer is $224 \\times 224 \\times 3$.\n",
    "A convolution of size $11 \\times 11$ with padding 2 and stride 4 gives an output feature map width and height of\n",
    "\n",
    "$$\\lfloor \\frac{w+2p-k_w}{s} + 1 \\rfloor = \\lfloor \\frac{224+2(2)-11}{4} + 1 \\rfloor = \\lfloor 55.25 \\rfloor = 55$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet model\n",
    "\n",
    "From class, we now know that the AlexNet model (without splitting across GPUs) looks like this:\n",
    "\n",
    "<img src=\"img/alexnet.png\" title=\"AlexNet (Alex et al.)\" style=\"width: 900px;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet Coding in PyTorch\n",
    "\n",
    "First, we import some necessary libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up Dataset objects and DataLoader objects to load images, transform them to 3x224x224, and batch them for training/testing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Set up preprocessing of CIFAR-10 images to 3x224x224 with normalization\n",
    "# using the magic ImageNet means and standard deviations. You can try\n",
    "# RandomCrop, RandomHorizontalFlip, etc. during training to obtain\n",
    "# slightly better generalization.\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Download CIFAR-10 and split into training, validation, and test sets\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=preprocess)\n",
    "\n",
    "# Split the training set into training and validation sets randomly.\n",
    "# CIFAR-10 train contains 50,000 examples, so let's split 80%/20%.\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [40000, 10000])\n",
    "\n",
    "# Download the test set. If you use data augmentation transforms for the training set,\n",
    "# you'll want to use a different transformer here.\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=preprocess)\n",
    "\n",
    "# Dataset objects are mainly designed for datasets that can't fit entirely into memory.\n",
    "# Dataset objects don't load examples into memory until their __getitem__() method is\n",
    "# called. For supervised learning datasets, __getitem__() normally returns a 2-tuple\n",
    "# on each call. To make a Dataset object like this useful, we use a DataLoader object\n",
    "# to optionally shuffle then batch the examples in each dataset. During training.\n",
    "# To keep our memory utilization small, we'll use 4 images per batch, but we could use\n",
    "# a much larger batch size on a dedicated GPU. To obtain optimal usage of the GPU, we\n",
    "# would like to load the examples for the next batch while the current batch is being\n",
    "# used for training. DataLoader handles this by spawining \"worker\" threads that proactively\n",
    "# fetch the next batch in the background, enabling parallel training on the GPU and data\n",
    "# loading/transforming/augmenting on the CPU. Here we use num_workers=2 (the default)\n",
    "# so that two batches are always ready or being prepared.\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up to execute on a particular GPU or the CPU only:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device mps\n"
     ]
    }
   ],
   "source": [
    "# Device 'cuda' or 'cuda:0' means GPU slot 0.\n",
    "# If you have more than one GPU, you can select other GPUs using 'cuda:1', 'cuda:2', etc.\n",
    "# In terminal (Linux), you can check memory using in each GPU by using command\n",
    "# $ nvidia-smi\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"AlexNet\" with the Sequential API\n",
    "\n",
    "PyTorch deep learning models come in (at least) two possible styles:\n",
    "\n",
    "<img src=\"img/NNinPytorch.png\" style=\"width: 500px;\" />\n",
    "\n",
    "1. The PyTorch Sequential API is very expressive when we have a straightforward sequence of operations to perform on the input.\n",
    "2. The PyTorch neural network Module allows more flexible transformations of inputs, combination of multiple inputs, generation of multiple outputs, and so on.\n",
    "\n",
    "We will see that AlexNet (at least a simple form of AlexNet) can mostly be expressed as a Sequential process, whereas GoogLeNet, with its\n",
    "parallel branches within Inception modules, requires the use of the Module API.\n",
    "\n",
    "### The model\n",
    "\n",
    "Let's express an AlexNet-like network using `torch.nn.Sequential`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple module to flatten a batched feature map tensor into a batched vector tensor\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        return x.view(batch_size, -1)\n",
    "\n",
    "\n",
    "# AlexNet-like model using the Sequential API\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "alexnet_sequential = nn.Sequential(\n",
    "    nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nn.AdaptiveAvgPool2d((6, 6)),\n",
    "    Flatten(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(256 * 6 * 6, 4096),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(4096, NUM_CLASSES),\n",
    ")\n",
    "\n",
    "# Move model to target device\n",
    "\n",
    "alexnet_sequential = alexnet_sequential.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, by the way, it would be useful to go back to the [AlexNet paper](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
    "and ask what is missing in this version of the model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function\n",
    "\n",
    "Next, let's write a function to train our model for some number of epochs. This one is adapted from the PyTorch tutorials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    dataloaders,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    num_epochs=25,\n",
    "    weights_name=\"weight_save\",\n",
    "    is_inception=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    train_model function\n",
    "\n",
    "    Train a PyTorch model for a given number of epochs.\n",
    "\n",
    "            Parameters:\n",
    "                    model: Pytorch model\n",
    "                    dataloaders: dataset\n",
    "                    criterion: loss function\n",
    "                    optimizer: update weights function\n",
    "                    num_epochs: number of epochs\n",
    "                    weights_name: file name to save weights\n",
    "                    is_inception: The model is inception net (Google LeNet) or not\n",
    "\n",
    "            Returns:\n",
    "                    model: Best model from evaluation result\n",
    "                    val_acc_history: evaluation accuracy history\n",
    "                    loss_acc_history: loss value history\n",
    "    \"\"\"\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    loss_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, num_epochs))\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over the train/validation dataset according to which phase we're in\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                # Inputs is one batch of input images, and labels is a corresponding vector of integers\n",
    "                # labeling each image in the batch. First, we move these tensors to our target device.\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero out any parameter gradients that have previously been calculated. Parameter\n",
    "                # gradients accumulate over as many backward() passes as we let them, so they need\n",
    "                # to be zeroed out after each optimizer step.\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Instruct PyTorch to track gradients only if this is the training phase, then run the\n",
    "                # forward propagation and optionally the backward propagation step for this iteration.\n",
    "\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    # The inception model is a special case during training because it has an auxiliary\n",
    "                    # output used to encourage discriminative representations in the deeper feature maps.\n",
    "                    # We need to calculate loss for both outputs. Otherwise, we have a single output to\n",
    "                    # calculate the loss on.\n",
    "                    if is_inception and phase == \"train\":\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4 * loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # Backpropagate only if in training phase\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Gather our summary statistics\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            epoch_end = time.time()\n",
    "\n",
    "            elapsed_epoch = epoch_end - epoch_start\n",
    "\n",
    "            print(\"{} Loss: {:.4f} Acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n",
    "            print(\"Epoch time taken: \", elapsed_epoch)\n",
    "\n",
    "            # If this is the best model on the validation set so far, deep copy it\n",
    "\n",
    "            if phase == \"val\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), weights_name + \".pth\")\n",
    "            if phase == \"val\":\n",
    "                val_acc_history.append(epoch_acc)\n",
    "            if phase == \"train\":\n",
    "                loss_acc_history.append(epoch_loss)\n",
    "\n",
    "        print()\n",
    "\n",
    "    # Output summary statistics, load the best weight set, and return results\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(\"Training complete in {:.0f}m {:.0f}s\".format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print(\"Best val Acc: {:4f}\".format(best_acc))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history, loss_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and loss function\n",
    "\n",
    "Before we start training, we need to set up an optimizer object and a loss function. Typical choices for the loss function:\n",
    "\n",
    "-   For regression problems, we would normally use `nn.MSELoss()`. The equation is:\n",
    "    $$MSE=\\frac{1}{N}\\sum_{i=1}^N(y_i-\\hat{y}_i)^2 $$\n",
    "-   For binary classification, we normally use `nn.BCELoss()`:\n",
    "    $$BCE=-\\frac{1}{N}\\sum_{i=1}^N y_i\\cdot \\log{\\hat{y}_i} + (1-y_i)\\cdot \\log(1 -{\\hat{y}_i})$$\n",
    "-   For multinomial classification, we most often use `nn.CrossEntropyLoss()`:\n",
    "    $$CE=-\\sum_{i=1}^C t_i\\cdot \\log(f(s)_i),$$\n",
    "    where $t_i$ and $s_i$ are the ground truth and the CNN prediction for each class $i$ in $1..C$.\n",
    "    An activation function (Sigmoid / Softmax) is usually applied to the scores before the CE Loss computation, so $f(s)_i$ refers to the nonlinear activation function application.\n",
    "\n",
    "For specialized needs, you can define your own loss function! We'll see examples of that later in the course.\n",
    "\n",
    "Typical choices for the optimizer:\n",
    "\n",
    "-   SGD: Scholastic gradient descent, works well for most cases but requires appropriate values for the learning, momentum, and weight decay. Given $\\alpha$ is learning rate, and $\\beta$ is momentum, the equation is\n",
    "\n",
    "$$V_t = \\beta V_{t-1} + (1-\\beta)\\nabla_wL(W,X,y)$$\n",
    "$$W_{t+1} = W_t + \\alpha V_t$$\n",
    "\n",
    "for more information please see [Stochastic Gradient Descent with momentum](https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d)\n",
    "\n",
    "-   Adam: adaptive learning rate optimizer that usually gives superior results to SGD but sometimes doesn't work. Adam's equations are:\n",
    "    $$(m_t)_i=\\beta_1(m_{t-1})_i+(1-\\beta_1)(\\nabla(W_t))_i,$$\n",
    "    $$(v_t)_i=\\beta_2(v_{t-1})_i+(1-\\beta_2)(\\nabla(W_t))_i^2,$$\n",
    "    $$(W_{t+1})i=(W_t)_i-\\alpha\\frac{\\sqrt{1-(\\beta_2)_i^t}}{1-(\\beta_i)_i^t}\\frac{(m_t)_i}{\\sqrt{(v_t)_i}+\\epsilon}$$\n",
    "-   See the many other choices selected from recent deep learning papers in the [PyTorch optim documentation](https://pytorch.org/docs/stable/optim.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossEntropyLoss for multinomial classification (because we have 10 classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# parameters = weights\n",
    "params_to_update = alexnet_sequential.parameters()\n",
    "# Use scholastic gradient descent for update weights in model with learning rate 0.001 and momentum 0.9\n",
    "optimizer = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Use train_model function for training. (bs128, GPU 1223MiB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m dataloaders \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_dataloader, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m: val_dataloader}\n\u001b[0;32m----> 3\u001b[0m best_model, val_acc_history, loss_acc_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43malexnet_sequential\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malex_sequential_lr_0.001_bestsofar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 100\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, criterion, optimizer, num_epochs, weights_name, is_inception)\u001b[0m\n\u001b[1;32m     97\u001b[0m     running_corrects \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m labels\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     99\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloaders[phase]\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m--> 100\u001b[0m epoch_acc \u001b[38;5;241m=\u001b[39m \u001b[43mrunning_corrects\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloaders[phase]\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m    101\u001b[0m epoch_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    103\u001b[0m elapsed_epoch \u001b[38;5;241m=\u001b[39m epoch_end \u001b[38;5;241m-\u001b[39m epoch_start\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead."
     ]
    }
   ],
   "source": [
    "dataloaders = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
    "\n",
    "best_model, val_acc_history, loss_acc_history = train_model(\n",
    "    alexnet_sequential,\n",
    "    dataloaders,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    10,\n",
    "    \"alex_sequential_lr_0.001_bestsofar\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet using the Module API\n",
    "\n",
    "The Sequential API makes it easy to create a sequential model, but not all models are sequential. For example, we need more flexibility\n",
    "to create a complex model such as GoogLeNet.\n",
    "\n",
    "Working with the `Module` API requires us to use object-oriented inheritance in Python. This means you'll have to brush up on your OO concepts or learn the basics if OOP is new to you.\n",
    "\n",
    "We create a new class that inherits from `Module`, then in most cases, we just need to override two methods: `__init__()` and `forward()`.\n",
    "\n",
    "`__init__()` is called the \"constructor\" of a class and is the method called on any Python object just after it is created, similar to constructors in Java or C++.\n",
    "\n",
    "However, constructors and instance methods work a little differently in Python than they do in Java or C++. The constructor is just an ordinary instance method that is only special in that it\n",
    "is called implicitly when the object is created. Instance methods in Python (methods called on an object) are distinguished from class methods (methods called on the class, not requiring any\n",
    "instance) by the presence or absence of the `self` keyword in the parameter list. In the body of an instance method, `self` is a reference to the instance the method was called on, same as\n",
    "`this` in Java or C++ or `self` in Ruby.\n",
    "\n",
    "Anther difference between Python and other languages is that object initialization in an inheritance hierarchy is more flexible.\n",
    "A constructor should normally call `super(ClassName, self).__init__()` (Python 2, also works in Python 3) or `super().__init__()` (Python 3 only) at the beginning of its\n",
    "own `__init__()` method to initialze any fields used by methods in the superclass, but it need not do so.\n",
    "\n",
    "In the case of a PyTorch `Module` subclass, we should call `super()` before doing other things.\n",
    "\n",
    "The `forward()` method is also an instance method that is implicitly called when we invoke a `Module` instance as a function. So the code\n",
    "\n",
    "    module = MyModule()\n",
    "\n",
    "creates an instance of `MyModule` and then calls its `__init__()` method, whereas\n",
    "\n",
    "    outputs = module(inputs)\n",
    "\n",
    "invokes the `forward()` method defined in `MyModule`.\n",
    "\n",
    "### The model\n",
    "\n",
    "Here's an implementation of an AlexNet-like network.\n",
    "\n",
    "Note that `Sequential` is itself a subclass of `Module`. This means we can use `Sequential` for a sequential flow in a larger network.\n",
    "\n",
    "Also note that the adaptive average pool layer between the feature module and the classifier is a trick used to ensure a fixed set of 6x6 feature maps come out\n",
    "of the feature extractor regardless of the input image size. It is not strictly required here (and not used in the original paper) but would allow us to use other input sizes besides 224x224 if we like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetModule(nn.Module):\n",
    "    \"\"\"\n",
    "    An AlexNet-like CNN\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    num_classes : int\n",
    "        Number of classes in the final multinomial output layer\n",
    "    features : Sequential\n",
    "        The feature extraction portion of the network\n",
    "    avgpool : AdaptiveAvgPool2d\n",
    "        Convert the final feature layer to 6x6 feature maps by average pooling if they are not already 6x6\n",
    "    classifier : Sequential\n",
    "        Classify the feature maps into num_classes classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int = 10) -> None:\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "(bs 128, GPU 2233MiB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet_module = AlexNetModule(10)\n",
    "alexnet_module = alexnet_module.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make optimizer and Loss function\n",
    "criterion_2 = nn.CrossEntropyLoss()\n",
    "params_to_update_2 = alexnet_module.parameters()\n",
    "optimizer_2 = optim.SGD(params_to_update_2, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model2, val_acc_history2, loss_acc_history2 = train_model(\n",
    "    alexnet_module,\n",
    "    dataloaders,\n",
    "    criterion_2,\n",
    "    optimizer_2,\n",
    "    10,\n",
    "    \"alex_module_lr_0.001_bestsofar\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results\n",
    "\n",
    "Based on these results, let's plot the validation loss/accuracy curves:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_data(val_acc_history, loss_acc_history):\n",
    "    plt.plot(loss_acc_history, label=\"Validation\")\n",
    "    plt.title(\"Loss per epoch\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.plot(val_acc_history, label=\"Validation\")\n",
    "    plt.title(\"Accuracy per epoch\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc_history2 = [t.cpu().item() for t in val_acc_history2]\n",
    "plot_data(val_acc_history2, loss_acc_history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogleLeNet\n",
    "\n",
    "Next, we'd like to construct GoogLeNet as described in the [original GoogLeNet paper](https://arxiv.org/abs/1409.4842) from scratch.\n",
    "\n",
    "This part of the lab is adapted from [kuangliu's PyTorch CIFAR repository on GitHub](https://github.com/kuangliu/pytorch-cifar/blob/master/models/googlenet.py).\n",
    "\n",
    "### GoogleLeNet\n",
    "\n",
    "GoogleLeNet or Inception network is an important concept for development CNN classifier. Most of CNNs just stacked convolution deeper and deeper to get performance, but very deep networks are prone to overfitting. It also hard to pass gradient updates through the entire network, and make computation expensive. In the other hands, inception network do in wider path to improve performance.\n",
    "\n",
    "<img src=\"img/expandDeeper.jpg\" style=\"width: 400px;\" />\n",
    "\n",
    "There are several versions of the inception networks such as Inception v1, Inception v2, Inception v3, Inception v4, and Inception-ResNet.\n",
    "\n",
    "The full architecture of GoogLeNet (inception1) looks like this:\n",
    "\n",
    "<img src=\"img/GoogleLeNet.png\" style=\"width: 1080px;\" />\n",
    "\n",
    "### Inception block\n",
    "\n",
    "The key innovation introduced by GoogLeNet is the concept of the \"inception\" block. A standard inception block looks like this:\n",
    "\n",
    "<img src=\"img/inception.png\" style=\"width: 600px;\" />\n",
    "\n",
    "### Auxiliary classifiers\n",
    "\n",
    "To prevent the middle part of the network from “dying out”, the authors introduced two auxiliary classifiers (The purple boxes in the image). They essentially applied softmax to the outputs of two of the inception modules, and computed an auxiliary loss over the same labels. The total loss function is a weighted sum of the auxiliary loss and the real loss. Weight value used in the paper was 0.3 for each auxiliary loss.\n",
    "\n",
    "$$ \\mathcal{L}_{total} = \\mathcal{L}_{Real} + 0.3 \\mathcal{L}_{aux_1} + 0.3 \\mathcal{L}_{aux_2}$$\n",
    "\n",
    "### Inception v1 coding\n",
    "\n",
    "Let's implement the architecture.\n",
    "Take a look at each element and see how it implements the concepts described in the paper.\n",
    "First, we begin with a `Module` for an inception block with parameters that can be customized to implement each block in the overall network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    \"\"\"\n",
    "    Inception block for a GoogLeNet-like CNN\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    in_planes : int\n",
    "        Number of input feature maps\n",
    "    n1x1 : int\n",
    "        Number of direct 1x1 convolutions\n",
    "    n3x3red : int\n",
    "        Number of 1x1 reductions before the 3x3 convolutions\n",
    "    n3x3 : int\n",
    "        Number of 3x3 convolutions\n",
    "    n5x5red : int\n",
    "        Number of 1x1 reductions before the 5x5 convolutions\n",
    "    n5x5 : int\n",
    "        Number of 5x5 convolutions\n",
    "    pool_planes : int\n",
    "        Number of 1x1 convolutions after 3x3 max pooling\n",
    "    b1 : Sequential\n",
    "        First branch (direct 1x1 convolutions)\n",
    "    b2 : Sequential\n",
    "        Second branch (reduction then 3x3 convolutions)\n",
    "    b3 : Sequential\n",
    "        Third branch (reduction then 5x5 convolutions)\n",
    "    b4 : Sequential\n",
    "        Fourth branch (max pooling then reduction)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n",
    "        super(Inception, self).__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.n1x1 = n1x1\n",
    "        self.n3x3red = n3x3red\n",
    "        self.n3x3 = n3x3\n",
    "        self.n5x5red = n5x5red\n",
    "        self.n5x5 = n5x5\n",
    "        self.pool_planes = pool_planes\n",
    "\n",
    "        # 1x1 conv branch\n",
    "        self.b1 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, n1x1, kernel_size=1),\n",
    "            nn.BatchNorm2d(n1x1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # 1x1 conv -> 3x3 conv branch\n",
    "        self.b2 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, n3x3red, kernel_size=1),\n",
    "            nn.BatchNorm2d(n3x3red),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n3x3),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # 1x1 conv -> 5x5 conv branch\n",
    "        self.b3 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, n5x5red, kernel_size=1),\n",
    "            nn.BatchNorm2d(n5x5red),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n5x5red, n5x5, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n5x5),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n5x5),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # 3x3 pool -> 1x1 conv branch\n",
    "        self.b4 = nn.Sequential(\n",
    "            nn.MaxPool2d(3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n",
    "            nn.BatchNorm2d(pool_planes),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.b1(x)\n",
    "        y2 = self.b2(x)\n",
    "        y3 = self.b3(x)\n",
    "        y4 = self.b4(x)\n",
    "        return torch.cat([y1, y2, y3, y4], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The whole shebang\n",
    "\n",
    "Now the whole shebang.\n",
    "\n",
    "Note that kiangliu's version is intended for CIFAR-10, so it's assuming a small input image size (3x32x32). Also, there are no side classifiers.\n",
    "In the exercises, you'll convert this to the ImageNet style 224x224 input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "    \"\"\"\n",
    "    GoogLeNet-like CNN\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    pre_layers : Sequential\n",
    "        Initial convolutional layer\n",
    "    a3 : Inception\n",
    "        First inception block\n",
    "    b3 : Inception\n",
    "        Second inception block\n",
    "    maxpool : MaxPool2d\n",
    "        Pooling layer after second inception block\n",
    "    a4 : Inception\n",
    "        Third inception block\n",
    "    b4 : Inception\n",
    "        Fourth inception block\n",
    "    c4 : Inception\n",
    "        Fifth inception block\n",
    "    d4 : Inception\n",
    "        Sixth inception block\n",
    "    e4 : Inception\n",
    "        Seventh inception block\n",
    "    a5 : Inception\n",
    "        Eighth inception block\n",
    "    b5 : Inception\n",
    "        Ninth inception block\n",
    "    avgpool : AvgPool2d\n",
    "        Average pool layer after final inception block\n",
    "    linear : Linear\n",
    "        Fully connected layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        self.pre_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 192, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.a3 = Inception(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "        self.a4 = Inception(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.b4 = Inception(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.c4 = Inception(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.d4 = Inception(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "\n",
    "        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
    "        self.linear = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pre_layers(x)\n",
    "        out = self.a3(out)\n",
    "        out = self.b3(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.a4(out)\n",
    "        out = self.b4(out)\n",
    "        out = self.c4(out)\n",
    "        out = self.d4(out)\n",
    "        out = self.e4(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.a5(out)\n",
    "        out = self.b5(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, here are the Dataset and DataLoader objects from kiangliu. Notice the transforms may be more suitable for CIFAR-10 than the ImageNet transforms we implemented last week. But will they work as well?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess inputs to 3x32x32 with CIFAR-specific normalization parameters\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(36),\n",
    "        transforms.CenterCrop(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Download CIFAR-10 and set up train, validation, and test datasets with new preprocess object\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=preprocess)\n",
    "\n",
    "train_datset, val_dataset = torch.utils.data.random_split(train_dataset, [40000, 10000])\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=preprocess)\n",
    "\n",
    "# Create DataLoaders\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "dataloaders = {\"train\": train_dataloader, \"val\": val_dataloader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training GoogLeNet\n",
    "\n",
    "(bs 128, GPU 8433MiB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenet = GoogLeNet().to(device)\n",
    "criterion_3 = nn.CrossEntropyLoss()\n",
    "params_to_update_3 = googlenet.parameters()\n",
    "optimizer_3 = optim.Adam(params_to_update_3, lr=0.01)\n",
    "\n",
    "best_model3, val_acc_history3, loss_acc_history3 = train_model(\n",
    "    googlenet,\n",
    "    dataloaders,\n",
    "    criterion_3,\n",
    "    optimizer_3,\n",
    "    25,\n",
    "    \"googlenet_lr_0.01_bestsofar\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc_history3 = [t.cpu().item() for t in val_acc_history3]\n",
    "plot_data(val_acc_history3, loss_acc_history3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Create these three networks. Be sure to properly define your Python classes, with one class per file and a main module that sets up your\n",
    "   objects, runs the training process, and saves the necessary data.\n",
    "2. Note that the AlexNet implementation here does not have the local response normalization feature described in the paper. Take a look at the\n",
    "   [PyTorch implementation of LRN](https://pytorch.org/docs/stable/generated/torch.nn.LocalResponseNorm.html) and incorporate it into your AlexNet implementation as\n",
    "   it is described in the paper. Compare your test set results with and without LRN.\n",
    "3. Note that the backbone of the GoogLeNet implemented thus far does not correspond exactly to the description. Modify the architecture to\n",
    "    1. Use the same backbone (input image size, convolutions, etc.) before the first Inception module\n",
    "    2. Add the two side classifiers\n",
    "4. Compare your GoogLeNet and AlexNet implementations on CIFAR-10. Comment on the number of parameters, speed of training, and accuracy of the two models on this dataset when\n",
    "   trained from scratch.\n",
    "5. Experiment with the pretrained GoogLeNet and AlexNet from the torchvision repository. Does it give better results on CIFAR-10 similar to what we found with AlexNet? Comment\n",
    "   on what we can glean from the results about the capacity and generalization ability of these two models.\n",
    "\n",
    "## The report\n",
    "\n",
    "Describe your experiments and their results. The report should be turned in on Teal Classroom before the deadline.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
